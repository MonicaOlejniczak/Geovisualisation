%!TEX root = ../report.tex

\todo{Purpose of the chapter
 Structure of the chapter
 Central themes of the chapter}

\section{User interface} {
\label{sec:user_interface}

	The graphical user interface (GUI) for this system has combined Google's Material Design with a 3D environment, to simplify the way users interact with the system.

	\subsection{Material Design} {
	\label{sec:material_design}

		Material Design is a visual language developed by Google that synthesises classic design principles with current technologies. Their design aims to provide users with a unified experience across platforms and device sizes, by focusing on user actions and retaining continuity between transitions~\footnote{\bibentry{google2015material}}. Furthermore, Material Design provides an immersive experience to users by adhering to good design principles.

		Google Material Design details many component specifications that have been implemented by several CSS3 frameworks. It is also a great design tool that has been incorporated into the existing infrastructure this project and the group project. As a result of this, a teacher can transition from one system to the other seamlessly, because both systems apply the same design standards.

	}

	\subsection{3D environment} {
	\label{sec:3d_environment}

		The visualisation being viewed by a user features a 3D environment. It has been designed with the following in mind:

		\begin{itemize}
			\item Navigation interactions -- pan, zoom and rotate.
			\item Data point displays that represent the dataset are displayed as 3D bars.
				\begin{itemize}
					\item Data displays should have the ability to be filtered.
				\end{itemize}
			\item The colour of a data display should be effected by the value it represents.
				\begin{itemize}
					\item e.g. Data displays that represent population data may be coloured red, orange or yellow to indicate high, medium or low population areas respectively.
				\end{itemize}
			\item Information associated with a data display can be viewed by a user.
				\begin{itemize}
					\item Should use a simple panel interface, shown in Figure~\ref{fig:information_display_design}.
					\item Information is retrieved through metadata. 
					\item Can be shown through hover effects or by outputting the dataset on the page.
				\end{itemize}
			\item Data displays appear projected from a base geometry.
				\begin{itemize}
					\item The geometry can be a cuboid or sphere and depends on the dataset being used.
				\end{itemize}
			\item A skybox should serve as a static background for the environment.
		\end{itemize}

		\input{figures/design/mockups/information_display}

		Example mockups of the 3D environment that fulfil the above criteria have been demonstrated in Figure~\ref{fig:environment_mockups}.

		\input{figures/design/mockups/3d_environment}

	}

	\subsection{Interface design} {
	\label{sec:interface_design}

		The design of the interface needs to account for both the use of Material Design and the 3D environment. 

		Material Design is renowned for its sliding drawer menu, often used to maximise the viewing space for the user. This component has the potential to maintain user focus on the visualisation, while providing additional information through an unobtrusive menu. An initial mockup of this design has been demonstrated in Figure~\ref{fig:user_interface_mockup}, where the 3D environment fills the content to the right of the drawer. This design has been refined through the implementation phase described in Section~\ref{sec:filtering_implementation} and \ref{sec:configuration_implementation}.

		\input{figures/design/mockups/user_interface}

		The sliding drawer utilises Material Design components and incorporates two main features -- filtering and live configuration.

		\subsubsection{Filtering} {
		\label{sec:filtering}

			Filters take advantage of slider and checkbox widgets to adjust the state of what is displayed to the user. Their purpose is to provide users with the ability to identify characteristics of the dataset more quickly.

			\emph{Sliders} adjust what data displays are visible to the user. This is achieved by configuring the bounds of the available metadata in the data model. Take for example a dataset that has a range of values between 0 -- 100. If the user adjusts the value of its respective slider to be $\ge$20, then any data displays that have a value \textless20 will be hidden.
			
			\emph{Checkboxes} are designed to be toggled on or off. This widget toggles what metadata associated with a data display is shown to the user.

		}

		\subsubsection{Configuration} {
		\label{sec:configuration}

			Configuration options are designed to change the values and attributes of the 3D environment in real-time, promoting the analysis of the effects colour has on usability and identifying pertinent information. These options are organised in a folder structure, to deliver a hierarchy and logical grouping to the user. Moreover, these options should handle configuring data displays and surface values which enable the user to customise the display of the system.

			\emph{Data display} configurations should modify the low, medium, high colour values and the range of colours displayed.

			\emph{Surface} configurations should modify the midtone colours of the surface and the colour or intensity of other surface attributes.

		}

	}

}

\section{User actions} {
\label{sec:user_actions}

	The interactions that a user can perform when using this system have been represented in Figure~\ref{fig:user_actions}.

	\input{figures/design/user_actions}

	These user actions facilitate decision making processes by allowing users to evaluate different datasets and identify key features. For instance, when the user navigates around the scene they are able to view the information associated with a data display by hovering on it. In regards to population data the smallest and largest city or country can be established using this method. This is achieved by navigating to a short or tall data point and displaying its information which contains details such as the city and country. Similarly, when a student dataset is used the best and worst performing groups can be identified on a weekly and on an average basis by analysing this information. Viewing student progress in such a manner can also aid in determining patterns with particular groups and individuals, and examining the overall complexity of a task during certain weeks. 

	When filtering is applied to any dataset, the user can quickly confirm their initial observations found during the navigate and view data point information actions. This is due to the fact that a filter can determine which data displays are visible to the user, by adjusting the minimum and maximum range of a property in the dataset through a widget. As an example, displaying populations that are either below or over 100,~000 individuals can easily be seen by adjusting the minimum or maximum filter bounds for the population property to 100,~000.

	Finally, different configurations can be applied in order to customise the appearance of the environment. This is done so that a distinguished colour scheme for the data displays and surface is selected. In turn, this can enhance a user's ability to identify significant features and patterns through colour palettes that make the dataset clearer to understand.

}

\section{Methodology} {
\label{sec:methodology}

	A rapid application development (RAD) approach was undertaken when developing this project as it promotes iterative development and the rapid construction of prototypes. This software methodology is flexible and can respond to changing requirements because less time is invested into planning the project. This contrasts with the traditional waterfall approach which places greater emphasis on planning, gathering requirements, scheduling and following a rigid workflow. RAD instead enables the software to be written quickly through the reuse of software components and engaging in less formal reviews. The RAD model has been shown in Figure~\ref{fig:rad} below.

	\input{figures/design/rad}

	In this project, each visualisation can be thought of as a single prototype, where parts of a prototype can be reused for the next visualisation. Testing should be performed during the course of the project, but particularly when a major component of a prototype has been developed, such as data filtering.
		
}

\section{Non-functional requirements} {
\label{sec:non_functiona_requirements}

	The following non-functional requirements are considered key to the success of this project:

	\begin{description}[leftmargin=0pt]
		\item[Modularity:] The project should be highly modular, so parts of the system can be reused and easily integrated into the group project.
		\item[Performance:] The response time of the system is a major concern when working with a large dataset. The time to load the visualisation is highly dependent on the size of the dataset, graphical processing, network bandwidth and latency. However, once the visualisation has loaded, the user should be faced with near immediate response times (\textless 1 second).
		\item[Scalability:] This is important when modifying the size of the dataset used with a visualisation. The visualisation should be capable of handling increased processing with a larger dataset. Not only this, but the information should remain mapped to the correct locations in the scene with increased volumes of data.
		\item[Testability:] Highly testable code is important when writing test suites, so all aspects of the system can be easily tested to locate faults. In JavaScript:
			\begin{itemize}
				\item A common technique is to hide variables through the use of closure, which leads to untestable code. These variables should be replaced with public variables that are prefixed with an underscore, to indicate they are not a part of the public API.
				\item Promises should not be used within a constructor.
				\item Anonymous functions should be replaced with named functions so they can be tested.
				\item Dependency injection should be used through RequireJS, so all dependencies can be mocked.
			\end{itemize}
	\end{description}

}

\section{Dataset} {
\label{sec:dataset}

	As discussed in Section~\ref{sec:project_definition}, the visualisations began with generated data so development could begin immediately. Using generated data brings control over the size of the dataset applied to a prototype and the scalability of each visualisation can be tested. 

	Following the use of fake data, each prototype had real datasets applied to them. GeoNames~\footnote{\bibentry{wick2005geonames}} is a geographical database that was used for the implementation of the visualisations. This database stores population data for a significant number of cities and their respective latitude and longitude coordinates. GeoNames falls under a creative commons attribution license, so there are no copyright concerns or immediate issues when reading and parsing the data it provides. However, some unnecessary information contained in this dataset (such as \emph{geonameid}, \emph{feature class} and \emph{dem}) were removed. Furthermore, the format of the data was modified from a tab separated list to a JSON structure so the data could be read efficiently in order to reduce the loading time for the user.

	Once real datasets had been applied to the visualisations, the system was integrated with the final year project to provide an analytics component for teachers. The progress made by tutorial groups were stored in the database of the group project. This data could be visualised to aid teachers in analysing ongoing student progress.

	Initially, a flat array structure was considered for the design of the dataset as it is significantly more performant to parse than objects, as shown in Figure~\ref{fig:performance_test}. However, this structure is inflexible and difficult to maintain as a result of its rigid format. The use of objects are a more flexible, albeit less performant, structure that was considered in the design of the dataset. When using objects, a key can be selected as a value to be used for the x, y and z axis. Another advantage to using objects is that metadata can easily be added to the dataset. While there are certainly tradeoffs between the two structures, the flexibility of objects was valued over performance differences that would only occur during application startup. Due to these reasons, objects were used to form the structure of the datasets.

	\input{figures/design/performance_test}

}

\section{Architecture} {
\label{sec:architecture}
	
	\todo{three-tier: presentation, logic, data

		inheritance

		design patterns

		forwards ajax request to backend of fyp system for dataset

		}

		\todo{Everything implemented as a single module, write more / reword}

		% The Filter, Configuration, and Information display modules are external to the core 3D environment components, separating the components in the scene from the view controllers. This design decision also ensures that the view controllers can be removed from the application at any time, without effecting the core functionality.

		\input{figures/design/architecture}

% 		The frontend system is a separate client-server application, consisting of a node.js server component, which simply pushes the Client-side component to the client-browser. The client-side therefore contains the vast majority of application complexity which has the following architecture:
% Uses a MVVM (Model View View-Model) architectural style to compose data bound from performing asynchronous requests to the backend API, with the view-templates via the View-Model (controllers).
% The API-Client 
% Much of the 'heavy lifting' of data manipulation is delegated to the backend API.
% View-templates are a set of DSL template files that compose a particular UI component
% These are dynamically compiled into views, incorporating data from the model.
% The primary function of the View-Model (depicted as controllers) is to bind changes to the Model data and update the Views accordingly.
% Conceptually this functionality requires a data-mapping sub-module, but it is likely that this will be provided by a library.
% This all occurs in an asynchronous Event-based manner, such that the UI is never blocked.


}

\section{Evaluation design} {
\label{sec:evaluation_design}

	It is important to measure the usability of the system and the tools it offers, so users can effectively interpret the information presented to them in an attempt to aid data exploration and decision-making processes. Performance plays a significant role toward the usability of the system, as it ultimately determines how responsive the system appears to the user. Both the usability and performance of the system can be assessed by performing a user study and performance evaluation respectively.

	The user study has been designed to include a questionnaire and System Usability Scale, which both aim to assess the usability of the system. The objective of the questionnaire is to collect information from the user regarding their experience when using the system. The user is also asked to perform two activities and answer whether they used filtering to discover this information. Similarly, the System Usability Scale~\parencite{brooke1996sus} is a tool for measuring the usability of a system. This scale consists of a simple ten item questionnaire and is designed to be answered by respondents quickly and easily. Furthermore, the System Usability scale has become an industry standard by yielding reliable results and can be viewed in Appendix~\ref{app:system_usability_scale}.

	This study has been limited to the students enrolled in the final year project. It is required that the participants sign a consent form before they evaluate the system. Likewise, the participants must be informed of how the confidentiality of the research data will be maintained. This will be supported by anonymising the research data such that the participants cannot be identified from the results presented in this thesis.

	The performance and scalability of the system has been designed to be tested in an offline environment on a single high performance computer. The metrics for evaluating the performance and scalability of the system include the time to render, time to perform filtering and the average frames per second when idle and navigating the scene. These metrics have been recorded for each visualisation across small ($\approx$ $\le1000$) and large ($\approx$ $\ge10000$) datasets.

}
