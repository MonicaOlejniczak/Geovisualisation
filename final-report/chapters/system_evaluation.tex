%!TEX root = ../report.tex

As discussed in Section~\ref{sec:evaluation_design} of the Design chapter, a user study and performance analysis was undertaken as a way to measure the usability and effectiveness of the visualisations that have been implemented in this project. The details for these evaluations have been described in Section~\ref{sec:user_study} and \ref{sec:performance_analysis} respectively.

The system evaluations were performed on an MSI GS60 Ghost Pro 4K 15.6in Gaming Notebook with 16GB RAM and an NVIDIA GeForce GTX 970M dedicated graphics card. The visualisations were loaded in an offline environment using the high performance mode offered in Windows 8.1 to enhance the user experience and minimise performance issues.

This chapter introduces the participants and process of the user study, outlines how the performance analysis was achieved, then presents and discusses the results of these evaluations.

\section{User study} {
\label{sec:user_study}

	Prior to commencing the user study, there were a couple user issues that were observed throughout the development process. As a result, it was expected that users may experience difficulty navigating the population globe and may be confused as to the purpose of the checkboxes in the sidebar.

	There were eight male participants who volunteered to undertake this user study. The participants were aged between 22-29 and are all enrolled in the final year project. All participants come from a technical background, use computers on a daily basis and are familiar with navigating websites and using common functionality such as data filtering.

	The process began by asking the participant to complete a consent form. A brief overview of each visualisation and the dataset they represent was provided after the user completed this form. The participant was given task instructions and encouraged to ask questions throughout the evaluation. They were then able to experiment with the visualisations in any order and asked to complete a questionnaire. On average the participant took approximately five minutes to complete the study.

}

\section{Performance analysis} {
\label{sec:performance_analysis}

	% \todo{performance slow with large data mention filtering and configuration}

	The performance analysis was conducted by carrying out performance testing, which has been outlined in Section~\ref{sec:performance_testing} of the Testing chapter. The tests were executed with a dataset size of 500, 1000, 5000, 10 000 and 20 000 for each visualisation. These tests were each performed three times and the average was then recorded, introducing another measure for increasing the accuracy of the results in the controlled environment.

}

\section{Results} {
\label{sec:results}

	\subsection{Questionnaire} {
	\label{sec:questionnaire}

		Half of the participants found the navigation controls cumbersome to use. In particular, participants commented that they could pan and zoom across the screen without experiencing any difficulty, but often struggled to rotate the scene. The rotation in the population globe was particularly troublesome to use by the participants. This is a direct result of the limits placed on the polar angle in the navigation, which were designed primarily for the cuboid surface. These limits prevent the user from rotating the sphere in a downward direction and thus the bottom could not be viewed adequately. 

		The system would benefit from having separate navigation controls for a visualisation using the sphere surface, by performing rotation on the sphere itself instead of rotating the camera around the scene. This change in the system would enable users to have complete control over the rotation of the sphere and hence all of the data points. It would also be worthwhile investigating other techniques implementing navigation, such as \href{http://threejs.org/examples/misc_controls_trackball.html}{trackball controls} to improve the usability of the navigation controls for the user.

		The majority of the participants (88\%) were able to correctly identify information in the population and student dataset through the tasks set in the questionnaire. In addition to this, all participants compared the height of the data points and the values shown in the information displays during these tasks. These participants also answered which visualisation they thought was the easiest to use and locate information in, as shown in Figure~\ref{fig:most_popular_visualisation}. 

		\input{figures/results/most_popular_visualisation}

		This result revealed that the population cuboid was the most popular gaining 63\% of total votes, while the population globe was favoured by 38\% of the participants. This can be attributed to the fact that the population globe featured poor navigation controls, which would greatly effect its usability and ability for users to locate information. Another factor for this result is that the data in the population cuboid is completely visible to the user when viewed from afar. In contrast to this, the data points in the population globe wrap around the surface and can appear hidden to the user until they rotate around the surface. 

		Unfortunately, the student grid was considered the least user-friendly. It was observed that some participants initially had trouble identifying what the student dataset represented. After some time, the participant could deduce this or instead asked for clarification. This issue could be solved by labelling the axes for a visualisation that uses a grid surface. It is also important to consider the way the information is presented to the user in this visualisation. The student grid displays highly clustered information and it can be challenging to identify information when filtering is not used. It would be valuable to explore methods for increasing the usability of this visualisation, especially in regards to how the sizes and spacing between the data points impact the interpretation of the dataset.

		Three-quarters (75\%) of the participants applied filtering when completing the activities presented in the questionnaire. Of those participants, 67\% applied filtering for the population dataset whereas the student dataset was filtered by all the participants that performed filtering. This finding strongly suggests that the participants found it more challenging to locate information in the student dataset, which is also supported in the previous result. During the observation of the participants, it was noted that those that applied filtering used it in one of two ways. The first way was to confirm the values they found when comparing the data in the information displays, while the second was to eliminate the potential task solutions by hiding out of range data points. The latter was particularly prevalent in the student grid, which featured highly clustered data points that can be difficult to interpret by the user without filtering.

		When observing the participant behaviour, it soon became obvious that the purpose of the checkboxes, used for toggling the data shown in the information displays, was unclear. The participants that attempted to use this feature often toggled the checkbox multiple times expecting it to effect the visibility of the data points. This feature should be clearly documented, through help text or tooltips, in the system and perhaps placed under a different heading such as \emph{information display control} to concisely convey the purpose of the feature to the user. Furthermore, these results indicate that the slider filters proved to be helpful tools when analysing the visualisation. On the other hand, the checkboxes often hindered the analysis of the visualisation since users did not know how to use the tool.

		Only some of the participants (38\%) applied custom configurations when answering the questions. The participants that did utilise this feature did not use it in a way that helped them interpret the data presented in the visualisation. This feature was instead used briefly by the participants as a way of experimenting with the output of the available shader effects. This result, when combined with the participant behaviour, implies either one of two things. The first is that the default colours used in the visualisations were suitable for interpreting the dataset, or that the colours did not have a significant impact on the usability of the system. However, this feature can still prove to be useful for other users and those who suffer from a visual impairment such as colour blindness. The latter could not be tested in this user study as no participants reported that they had any significant visual impairments.

	}

	\subsection{System Usability Scale} {
	\label{sec:sus_results}

		The average System Usability Scale score for the system was 71.56, which ranks higher than the average~\parencite{brooke2013sus} SUS score of 68. This indicates that the system demonstrated good usability, but could still be improved thoroughly. Moreover, the overall SUS score across all participants has been shown in Figure~\ref{fig:sus_score_participants}.

		\input{figures/results/sus_score}

		In Figure~\ref{fig:sus_score_questions}, the SUS score across all questions is shown. From these results, it can be concluded that consistency is the greatest strength in the system by receiving the highest average SUS score of 87.5. Another asset was, aside from a single outlier, that 87.5\% of participants did not need to learn many things before they could begin using the system. However, the greatest downfalls of the system are that the participants did not feel confident using the system, nor would they use the system frequently. Both of these scores fell below the average SUS score with confidence measuring at 65.63 and future usability at 56.25. While participants felt that they would not use the system frequently, this result was expected given the demographic of the participants as they are not concerned with the field of data analytics or analysing the datasets presented to them.

		\input{figures/results/normalised_sus_score}

		In addition to the above results, it was almost unanimously agreed upon by the participants that the system was easy to use given the above average score of 75. While this score is representative of a user-friendly system, only one of the participants strongly agreed that it was easy to use and thus the usability of the system could still be improved greatly. 

		On the whole participants did not think the system was cumbersome to use and disagreed when asked if they thought the system was unnecessarily complex. These metrics both received a passable SUS score of 68.75, which is considered just above average. Similarly, the majority of the participants, totalling 75\% for both agree and strongly agree, concluded that other people would learn to use this system very quickly.

		The participants were also asked to consider if they would need the support of a technical person to use the system and if they thought the various functions of the system were well integrated. The responses for both criteria were just above average and sat at 71.88, where participants ranged greatly in regards to needing technical support and either remained neutral, agreed or strongly agreed when responding to the integration of system functions. This score could be improved with the integration of an in-built tutorial and help elements.

	}

	\subsection{Performance} {
	\label{sec:performance}

		The average startup time for each visualisation has been shown in Figure~\ref{fig:average_startup_time}. This graph shows that the startup times for the population cuboid and globe are very similar, whereas the student grid overall performed better. This is likely the case because the student grid does not load or process the surface shaders and textures such as the earth or clouds.

		\input{figures/results/average_startup_time}

		Moreover, the application load time increases significantly as the size of the dataset increases. This is an expected result from the system, however the rate of increase is alarming considering that this test was performed in an offline environment. While the application load time may be acceptable to a user expecting to view a large dataset in a 3D visualisation, an effort should be made to reduce the average startup time. Figure~\ref{fig:aggregated_startup_events} demonstrates the average aggregation of the startup events from each visualisation. This chart shows that the loading of scripts and resources is almost negligible as the system was loaded offline. It is also evident that the majority of the application startup is involved with processing scripts.

		\input{figures/results/startup_events}

		As a way of reducing the average startup time presented above, script processing should be minimised. One method that can achieve this is to offload client-side processing of the dataset to the Node.js server. This would allow the dataset to be loaded immediately into a Backbone collection as the majority of processing would then be performed offline. In this event, the application would only iterate through the dataset once, instead of twice and would drastically improve the load performance. 

		% This method can also be used to offload the processing of shaders, as a utility was written to asynchronously load and process import statements in shaders. Instead, these shaders can be loaded directly in the application

		In Figure~\ref{fig:average_fps}, the average frames rendered in the last second for the system is shown with each visualisation. Ideally, the system should attain 60FPS and will be used as the benchmark for top performance.

		\input{figures/results/average_fps}

		The average FPS was compared when the system was idle, during navigation interactions and with both an enabled and disabled raycaster. There were no perceived differences in the FPS rate between these states across all visualisations. There were however noticeable differences in the FPS rate that were based on the viewing angle and position of the camera as well as the amount of visible data points. Since the orientation of the camera was important when observing the FPS rate, the data was averaged from similar camera positions and rotations to reduce the error margin and maximise the consistency of the FPS results. 

		It was observed that a much higher FPS rate (\textgreater20 difference) was displayed when the student grid was viewed from the top in large datasets. This occurred because the WebGLRenderer renders fewer data points, which also appeared shorter due to the angle of the camera. A similar change in the FPS existed in both the population cuboid and globe, demonstrating the effects of the camera position with the FPS rate mentioned above. 

		It is also clear from Figure~\ref{fig:average_fps} that the FPS, and hence the performance and responsiveness of the system, rapidly declined with larger datasets. For instance, the visualisations were virtually unusable when loading a dataset of size 20, 000 and 4FPS (6.67\% of ideal FPS) but was still manageable with a size of 10, 000 and 8FPS (13.33\% of ideal FPS). The student grid was overall less performant than the population cuboid and globe which both performed equally. The lower FPS rates in the student grid can be explained by the need for the renderer to render high density data points. Furthermore, this result also concludes that the additional textures in the population datasets effectively made no difference in regards to the FPS of the system. However, these textures instead impact the application startup time and would add even greater load times when the network overhead is considered to download these images.

		Overall, the FPS of the system could be improved significantly. One method that facilitates this is the merging of data point geometries. This would greatly improve performance by limiting the amount of draw calls the renderer needs to execute, but also loses many of the benefits that individual meshes have. For example, the raycaster would no longer work and another object picking method would need to be used such as an octree raycaster. In the future, object picking methods should be further explored so a merged geometry can be used to boost the performance of the system.

	}

}

This chapter has described the process undergone through the user study and performance analysis and presented and discussed the results of these findings. It was found that the system was overall easy to use and very consistent. However, users did not feel confident using the system and many features could be improved to provide a more positive user experience. The small participant pool and restricted demographic could attribute to some of these results and should be taken into consideration. 

The usability results also determined that slider filters aid the analysis of geospatial data while the colours used in the geovisualisation did not have a significant impact on the usability of a geovisualisation. Instead, the height of the data points, filters and information displays helped with the analysis of the visualisations. 

Similarly, the performance analysis revealed that the application startup time and run-time performance declined significantly as the size of the dataset increased. The overall frame rate became too low for dataset sizes of $\geqslant5000$, while the average startup time became high for sizes of $\geqslant10, 000$ and should be improved in the future.
